---
editor_options:
  markdown:
    wrap: 72
output: rmarkdown::github_document
---

**University of Edinburgh**

**School of Mathematics**

**Bayesian Data Analysis, 2021/2022, Semester 2**

**Author: Aditya Prabaswara Mardjikoen (S2264710)**

**Lecturer: Daniel Paulin**

**Assignment 2**

**IMPORTANT INFORMATION ABOUT THE ASSIGNMENT**

**In this paragraph, we summarize the essential information about this
assignment. The format and rules for this assignment are different from
your other courses, so please pay attention.**

**1) Deadline: The deadline for submitting your solutions to this
assignment is the 11 April 12:00 noon Edinburgh time.**

**2) Format: You will need to submit your work as 2 components: a PDF
report, and your R Markdown (.Rmd) notebook. There will be two separate
submission systems on Learn: Gradescope for the report in PDF format,
and a Learn assignment for the code in Rmd format. You are encouraged to
write your solutions into this R Markdown notebook (code in R chunks and
explanations in Markdown chunks), and then select Knit/Knit to PDF in
RStudio to create a PDF report.**

![](knit_to_PDF.jpg){width="191"}

**It suffices to upload this PDF in Gradescope submission system, and
your Rmd file in the Learn assignment submission system. You will be
required to tag every sub question on Gradescope. A video describing the
submission process will be posted on Learn.**

**Some key points that are different from other courses:**

**a) Your report needs to contain written explanation for each question
that you solve, and some numbers or plots showing your results.
Solutions without written explanation that clearly demonstrates that you
understand what you are doing will be marked as 0 irrespectively whether
the numerics are correct or not.**

**b) Your code has to be possible to run for all questions by the Run
All in RStudio, and reproduce all of the numerics and plots in your
report (up to some small randomness due to stochasticity of Monte Carlo
simulations). The parts of the report that contain material that is not
reproduced by the code will not be marked (i.e. the score will be 0),
and the only feedback in this case will be that the results are not
reproducible from the code.**

![](run_all.jpg){width="376"}

**c) Multiple Submissions are allowed BEFORE THE DEADLINE are allowed
for both the report, and the code. However, multiple submissions are NOT
ALLOWED AFTER THE DEADLINE. YOU WILL NOT BE ABLE TO MAKE ANY CHANGES TO
YOUR SUBMISSION AFTER THE DEADLINE. Nevertheless, if you did not submit
anything before the deadline, then you can still submit your work after
the deadline. Late penalties will apply unless you have a valid
extension. The timing of the late penalties will be determined by the
time you have submitted BOTH the report, and the code (i.e. whichever
was submitted later counts).**

**We illustrate these rules by some examples:**

**Alice has spent a lot of time and effort on her assignment for BDA.
Unfortunately she has accidentally introduced a typo in her code in the
first question, and it did not run using Run All in RStudio. - Alice
will get 0 for the whole assignment, with the only feedback "Results are
not reproducible from the code".**

**Bob has spent a lot of time and effort on his assignment for BDA.
Unfortunately he forgot to submit his code. - Bob will get no personal
reminder to submit his code. Bob will get 0 for the whole assignment,
with the only feedback "Results are not reproducible from the code, as
the code was not submitted."**

**Charles has spent a lot of time and effort on his assignment for BDA.
He has submitted both his code and report in the correct formats.
However, he did not include any explanations in the report. Charles will
get 0 for the whole assignment, with the only feedback "Explanation is
missing."**

**Denise has spent a lot of time and effort on her assignment for BDA.
She has submitted her report in the correct format, but thought that she
can include her code as a link in the report, and upload it online (such
as Github, or Dropbox). - Denise will get 0 for the whole assignment,
with the only feedback "Code was not uploaded on Learn."**

**3) Group work: This is an INDIVIDUAL ASSIGNMENT, like a 2 week exam
for the course. Communication between students about the assignment
questions is not permitted. Students who submit work that has not been
done individually will be reported for Academic Misconduct which can
lead to serious consequences. Each problem will be marked by a single
instructor, so we will be able to spot students who copy.**

**4) Piazza: You are NOT ALLOWED to post questions about Assignment
Problems visible to Everyone on Piazza. You need to specify the
visibility of such questions as Instructors only, by selecting Post to /
Individual students/Instructors and type in Instructors and click on the
blue Instructors banner that appears below**

![](piazza_instructors.jpg)

**Students who post any information related to the solution of
assignment problems visible to their classmates will**

**a) have their access to Piazza revoked for the rest of the course
without prior warning, and**

**b) reported for Academic Misconduct.**

**Only questions regarding clarification of the statement of the
problems will be answered by the instructors. The instructors will not
give you any information related to the solution of the problems, such
questions will be simply answered as "This is not about the statement of
the problem so we cannot answer your question."**

**THE INSTRUCTORS ARE NOT GOING TO DEBUG YOUR CODE, AND YOU ARE ASSESSED
ON YOUR ABILITY TO RESOLVE ANY CODING OR TECHNICAL DIFFICULTIES THAT YOU
ENCOUNTER ON YOUR OWN.**

**5) Office hours: There will be two office hours per week (Monday
16:00-17:00, and Wednesdays 16:00-17:00) during the 2 weeks for this
assignment. The links are available on Learn / Course Information. We
will be happy to discuss the course/workshop materials. However, we will
only answer questions about the assignment that require clarifying the
statement of the problems, and will not give you any information about
the solutions. Students who ask for feedback on their assignment
solutions during office hours will be removed from the meeting.**

**6) Late submissions and extensions: Students who have existing
Learning Adjustments in Euclid will be allowed to have the same
adjustments applied to this course as well, but they need to apply for
this BEFORE THE DEADLINE on the website**

<https://www.ed.ac.uk/student-administration/extensions-special-circumstances>

**by clicking on "Access your learning adjustment". This will be
approved automatically.**

**For students without Learning Adjustments, if there is a justifiable
reason (external circumstances) for not being able to submit your
assignment in time, then you can apply for an extension BEFORE THE
DEADLINE on the website**

<https://www.ed.ac.uk/student-administration/extensions-special-circumstances>

**by clicking on "Apply for an extension". Such extensions are processed
entirely by the central ESC team. The course instructors have no role in
this decision so you should not write to us about such applications. You
can contact our Student Learning Advisor, Maria Tovar Gallardo
([maria.tovar\@ed.ac.uk](mailto:maria.tovar@ed.ac.uk){.email}) in case
you need some advice regarding this.**

**Students who submit their work late will have late submission
penalties applied by the ESC team automatically (this means that even if
you are 1 second late because of your internet connection was slow, the
penalties will still apply). The penalties are 5% of the total mark
deduced for every day of delay started (i.e. one minute of delay counts
for 1 day). The course intructors do not have any role in setting these
penalties, we will not be able to change them.**

![](fishing.jpg){width="100%"}

**Problem 1 - Fishing in a park**

**Our dataset from UCLA OARC has data on 250 groups that went to a
park.They were questioned about how many fish they caught (count), how
many people were in the group (persons), how many children were in the
group (child), whether or not they brought a camper van to the park
(camper), and whether or not they used live bait (livebait).**

**We are going to apply several regression models on this dataset. The
first step is to load the dataset and JAGS.**

```{r, message = FALSE, warning = FALSE}
library(rjags)
#We load JAGS

#You may need to set the working directory first before loading the dataset
#setwd("/Users/dpaulin/Dropbox/BDA_2021_22/Assignments/Assignment2")
fish=read.csv("fish.csv")
#The first 6 rows of the dataframe
print.data.frame(fish[1:6,])
```

**a)[10 marks]**

**Create a new column in the dataset for proportion of children in the
group (prop.child). Fit a Poisson GLM with log link function on the
number of fish caught (count) as response, and the covariates camper,
livebait, and prop.child as covariates. Use the offset feature of the
Poisson GLM to take the number of persons into account. Interpret the
results and discuss the meaning of the regression coefficients for the 3
covariates.**

Explanation:

Observe that in the dataset we does not have the data regarding the
proportion of children in the group (prop.child). We only have the data
of the number of children and people in each groups. By using this two
data, we can calculate the proportion of children in the group, which is
the number of children in the group per the number of people in a group.
After we compute the proportion of children in each group then we will
fit a Poisson GLM model with log link function on the number of fish
caught (count) as the response variable. In this Poisson GLM, the
variables that we are going to use as the explanatory variables is
prop.child, camper, and livebait. Since we use the $\verb|offset|$
function in $\verb|R|$ to take the number of persons into account into
the model, then we can formulate our Poisson GLM as
$$\log{(\mu)} = \beta_0+\beta_1\text{camper}+\beta_2\text{livebait}+\beta_3\text{prop.child}+\log{(\text{persons})},$$where
$\mu$ is the average number of fish that was caught, $\beta_0$ is the
Poisson GLM intercept, $\beta_1$ is the coefficient of the binary
variable that indicates whether or not a group brought a camper van to
the park ( 0 = no and 1 = yes), $\beta_2$ is the coefficient of the
binary variable that indicates whether or not a group use live baits to
capture the fish ( 0 = no and 1 = yes), and $\beta_3$ is the coefficient
of the proportion of children variable. The Poisson GLM that we
formulated for this problem can be written also as
$$\log{\left(\dfrac{\mu}{\text{persons}}\right)} = \beta_0+\beta_1\text{camper}+\beta_2\text{livebait}+\beta_3\text{prop.child}$$By
using the $\verb|R|$ code below, we will fit this Poisson GLM.

```{r}
# Calculate proprotion of children in the group
fish$prop.child = fish$child/fish$persons

#Fit and display model summary of Poisson GLM
m1.log <- glm(count ~ camper+livebait+prop.child+offset(log(persons)),data=fish,
              family=poisson)
summary(m1.log)
```

From the output above, we obtain $\hat{\beta_0} = -1.36776$,
$\hat{\beta_1} = 0.93438$, $\hat{\beta_2} = 1.77831$, and
$\hat{\beta_3} = -4.89927$. Therefore, we can write our Poisson GLM as
$$\log{\left(\dfrac{\hat{\mu}}{\text{persons}}\right)} = -1.36776+0.93438*\text{camper}+1.77831*\text{livebait}-4.89927*\text{prop.child},$$where
$\hat{\mu}$ is the estimated average number of fish that was caught.
After we obtained our Poisson GLM model, we will interpret the model
estimated parameters. First, lets interpret the Poisson GLM intercept.
We can interpret the regression intercept as the log of the average
number of fish that was caught per the total number of people in a
groups when all covariates in the model equals to zero. In other words,
the average number of fish that was caught per the total number of
people in a groups when there are no children in the groups, the groups
does not bring any camper van to the park, and the groups does not use
any live baits to catch the fish is $e^{\hat{\beta_0}}= 0.2546768$ .

Next, lets interpret the coefficient of the camper variable. The
interpretation for this parameter is that a one unit change in the
camper variable adds log of average number of fish that was caught per
the total number of people in a groups by 0.93438 assuming other
independent variables in the model remains constant. Since the group who
does not bring a camper van in the park is keyed as 0 while the group
that bring it is keyed as 1, this means that the average number of fish
that was caught per the total number of people in a groups increased by
$e^{\hat{\beta_1}} = 2.545635$ times when the group bring a camper van
in the park assuming that other independent variables in the model
remains constant.

Now lets interpret the coefficient of the livebait variable. The
interpretation for this parameter is that a one unit change in the
livebait variable adds log of average number of fish that was caught per
the total number of people in a groups by 1.77931 assuming other
independent variables in the model remains constant. Since the group who
does not use live baits to catch the fish is keyed as 0 while used it is
keyed as 1, this means that the average number of fish that was caught
per the total number of people in a groups increased by
$e^{\hat{\beta_2}} = 5.925766$ times when the group use live baits to
catch the fish assuming that other independent variables in the model
remains constant.

Finally, lets interpret the coefficient of the proportion of children
variable. The interpretation for this parameter is that a one unit
change in the proportion of children variable adds log of average number
of fish that was caught per the total number of people in a groups by
-4.89927 assuming other independent variables in the model remains
constant. This implies that the average number of fish that was caught
per the total number of people in a groups increased by
$e^{\hat{\beta_3}} = 0.007452021$ times for every one unit change in the
proportion of children variable assuming that other independent
variables in the model remains constant. The value of
$e^{\hat{\beta}_i}$, for $i = 0,1,\dots,3$ that we used here can be
obtained using the $\verb|R|$ code below.

```{r}
# Exponential values of estimated regression parameter
exp(coef(m1.log))
```

Poisson GLMs assume that the mean and variance of the response variable
increase at the same rate (see the model summary output above and the
statement $\verb|Dispersion|$ $\verb|parameter|$ $\verb|for|$
$\verb|poisson|$ $\verb|family|$ $\verb|taken|$ $\verb|to|$ $\verb|be|$
$\verb|1|$). This assumption must be confirmed. If the residual deviance
of the fitted model is bigger than the residual degrees of freedom, then
we have over-dispersion. Over-dispersion means that a Poisson
distribution does not adequately model the variance and is not
appropriate for the analysis. The over-dispersion statistics (residual
deviance divided by residual degrees of freedom) can be calculated using
the $\verb|R|$ code below:

```{r, message = FALSE, warning = FALSE}
# Calculate over-dispersion statistics
cat('Over-dispersion statistics:',m1.log$deviance/m1.log$df.residual)
```

From the output above, we can see that the over-dispersion statistics is
greater than 1. This implies that the residual deviance greater than the
residual deviance of the fitted model is bigger than the residual
degrees of freedom. Therefore, our Poisson GLM is not appropriate for
estimating the number of fish that was caught since this model violated
the assumption of the equality of mean and variance of Poisson
distribution. The explanation that I give in this part was based on the
GLMs in R for Ecology written by Carl Smith and Mark Warren. This is the
following link to access the pdf file:
<http://irep.ntu.ac.uk/id/eprint/37478/1/14596_Smith.pdf>

**b)[10 marks] Using JAGS, implement a Bayesian version of the model in
part a), i.e. a Bayesian Poisson GLM with log link function and the
number of persons acting as the offset variable. Explain how did you
construct your model. Use a Gaussian prior with mean 0 and variance 100
for the intercept, and independent Gaussian priors with mean 0 and
variance 1 for the other 3 regression coefficients. Do 5000 burn-in
iterations, and obtain 10000 samples from all regression coefficients
from this model using coda.samples.**

Explanation:

Previously in part a) we have formulated our Poisson GLM model. This
model can be formulated as
$$\log{(\mu)} = \beta_0+\beta_1\text{camper}+\beta_2\text{livebait}+\beta_3\text{prop.child}+\log{(\text{persons})},$$where
$\mu$ is the average number of fish that was caught. However, we have
not give further detail how to construct this model. In this part, we
will have a further detail about the derivation of Poisson GLM in part
a). Assume that the distribution of the response variable (the number of
fish that was caught) is a Poisson distribution with rate $\mu$. This
implies that $E[\text{count}] = \mu$, where in this case we can
interpret $\mu$ as the average number of fish that was caught.
Therefore, we can interpret $\frac{\mu}{\text{persons}}$ as the average
number of fish that was caught per the total number of persons in a
group (which represented by the persons variable in the fish.csv
dataset). Recall that in the Poisson GLM the link function is the log
link function, which is $g(\mu) = \log{(\mu)}$ where $\mu$ is the
average number of fish that was caught. Since the Poisson distribution
is a member of the exponential families, then we can treat the
regression problem with the response variable having a Poisson
distribution as a GLM problem. We are going to formulate our Poisson GLM
that shows the relationship between the average number of fish that was
caught per the total number of persons in a group with some explanatory
variable in the fish.csv dataset. We choose prop.child, camper, and
livebait as our explanatory variable in our Poisson GLM. By using the
link function $g(\mu) = \log{(\mu)}$ on the average number of fish that
was caught per the total number of persons in a group, we can formulate
our Poisson GLM as
$$\log{\left(\dfrac{\mu}{\text{persons}}\right)} = \beta_0+\beta_1\text{camper}+\beta_2\text{livebait}+\beta_3\text{prop.child},$$
which is equivalent with
$$\log{(\mu)} = \beta_0+\beta_1\text{camper}+\beta_2\text{livebait}+\beta_3\text{prop.child}+\log{(\text{persons})}.$$

Next, we will construct the Poisson GLM based on the Bayesian approach.
Since we use Poisson GLM here, then the response variable (number of
fish that was caught) is having a Poisson distribution with mean $\mu$.
In the Bayesian approach, we have to decide the prior distribution for
$\beta_i$ that we are going to use in the model, where
$i = 0,1,\dots,3$. We will use the Gaussian prior with mean 0 and
variance 100 for the intercept. As for the 3 regression coefficient in
the model, we will use Gaussian prior with mean 0 and variance 1. Let
$\mu_i$ and $\sigma^2_i$ respectively be the mean and variance parameter
for the Gaussian prior distribution for $\beta_i$, where
$i = 0,1,\dots,3$. In addition, let
$$\mathbf{x}_i = \left(\text{camper}_i, \text{livebait}_i, \text{prop.child}_i, \text{persons}_i\right),$$where
$\text{camper}_i$, $\text{livebait}_i$, $\text{prop.child}_i$, and
$\text{persons}_i$ each represent the i-th values of the observations
for the camper, livebait, prop.child, and persons variable respectively
for $i = 1,2,\dots,250$. Recall that there are 250 groups in the
$\verb|fish.csv|$ dataset. We can formulate our Bayesian Poisson GLM as
follows: $$\begin{aligned}
\text{count}_i|\mu_i, \mathbf{x}_i&\sim
\text{Poisson}(\mu_i), \quad i = 1,2,\dots, 250\\
\log{(\mu_i)} &= \beta_0+\beta_1\text{camper}_i+\beta_2\text{livebait}_i+\beta_3\text{prop.child}_i+\log{(\text{persons}_i)}\\
\beta_0 &\sim \text{N}(\mu_0 = 0, \sigma^2_0 = 100)\\
\beta_1 &\sim \text{N}(\mu_1 = 0, \sigma^2_1 = 1)\\
\beta_2 &\sim \text{N}(\mu_2 = 0, \sigma^2_2 = 1)\\
\beta_3 &\sim \text{N}(\mu_3 = 0, \sigma^2_3 = 1)
\end{aligned}$$

We have already formulated our Bayesian Poisson GLM model. Now, we will
explain how we can create this model in JAGS. Recall that in JAGS the
normal distribution function $\verb|dnorm|$ is parameterize in terms of
mean and precision. Therefore, we need to calculate the precision
parameter of the Gaussian prior distribution for each $\beta_i$,
$i = 0,1,\dots,3$. Let $\tau_i$ be the precision parameter of the
Gaussian prior distribution of $\beta_i$. By using the relationship
between precision and variance, we can formulated the precision
parameter as $\tau_i = \frac{1}{\sigma^2_i}$ for each $i = 0,1,\dots,3$.
Since $\sigma^2_0 = 100$ and $\sigma^2_i = 1$, for $i = 1,2,3$, then
$\tau_0 = 0.01$ and $\tau_i = 1$, for $i = 1,2,3$. The $\verb|R|$ code
below implements the JAGS model of the Bayesian Poisson GLM that we have
already formulated in this part. We ran 13 chains with 10000 samples
after 5000 burn-in iterations.

```{r}
# ---- Bayesian model
# Model is now Poisson(mu[i] = exp(b1*camper[i]+b2*livebait[i]+b3*prop.child[i])persons[i])

# Input data and prior hyperparameter for each beta
model1.data <- list(n=nrow(fish),camper=fish$camper,livebait=fish$livebait,
                    prop.child=fish$prop.child,persons=fish$persons, count=fish$count,
                    mu.beta0=0,mu.beta1=0, mu.beta2=0, mu.beta3=0, prec.beta0=1/100, prec.beta1=1,
                        prec.beta2=1, prec.beta3=1)

# Create model block for JAGS
model1_string <- "model {
# Data that will be read in are n, count, camper, livebait, prop.child and persons

# Prior for each parameter beta
beta0 ~ dnorm(mu.beta0,prec.beta0)
beta1 ~ dnorm(mu.beta1,prec.beta1)
beta2 ~ dnorm(mu.beta2,prec.beta2)
beta3 ~ dnorm(mu.beta3,prec.beta3)

#Likelihood
for(i in 1:n) {

# Poisson GLM
log(mu[i]) <- beta0+beta1*camper[i]+beta2*livebait[i] +beta3*prop.child[i]+log(persons[i])

# Observations
count[i] ~ dpois(mu[i])

# Replicates
count.rep[i] ~ dpois(mu[i])}
}"


# Compile the model
model1<- jags.model(file=textConnection(model1_string),data=model1.data,n.chains=13,quiet=T)

# Burnin 5000 iterations
update(model1, n.iter=5000)

# Generate posterior sample
model_param <- c("beta0","beta1","beta2", "beta3")
model1.samps <- coda.samples(model1,n.iter=10000, variable.names=model_param)
```

**c)[10 marks] Based on your MCMC samples, compute the Gelman-Rubin
convergence diagnostics (Hint: you need to run multiple chains in
parallel for this by setting the n.chains parameter). Discuss how well
has the chain converged to the stationary distribution based on the
results.**

**Compute and print out the effective sample sizes (ESS) for the
intercept and 3 regression coefficients.**

**If the ESS is below 1000 for any of these 4 parameters, increase the
sample size/number of chains until the ESS is above 1000 for all of
them.**

**Print out the summary of the fitted JAGS model. Compare this with the
results from the GLM model of a).**

**Check the sensitivity of the summary statistics with respect to the
priors.**

Explanation:

As we can see from either the $\verb|gelman.diag|$ or
$\verb|gelman.plot|$ results, the Gelman-Rubin diagnostics values are
below 1.05 for each of the Poisson GLM regression parameters in part b),
indicating that we have approximated the stationary distribution quite
well.

```{r}
# Gelman rubin plot
gelman.plot(model1.samps[,c('beta0','beta1')])
gelman.plot(model1.samps[,c('beta2','beta3')])

# Gelman rubin statistics
gelman.diag(model1.samps)
```

In our code in part b), we ran 13 chains with 10000 samples after 5000
burn-in iterations. We have computed the ESS (Effective Sample Size) for
the model parameters $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$
using the $\verb|effectiveSize|$ function. The ESS is above 1000 for
each of them, so no further samples are needed.

```{r}
# Effective sample size
effectiveSize(model1.samps)
```

Lets have a look at the trace plot of the Poisson GLM parameter
$\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ using the $\verb|R|$ code
below. From the trace plot below, we can see that the chain for the
parameter $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ were mixing
well and have converged.

```{r}
# Plot trace plot and density plot
plot(model1.samps[,c('beta0','beta1')])
plot(model1.samps[,c('beta2','beta3')])
```

Next, we printed out the posterior summaries for the Poisson GLM
parameters $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ using the
$\verb|summary(model1.samps)|$ command. We can see that the posterior
standard deviations for these four parameters are typically much smaller
that the posterior means in absolute value, indicating that the amount
of data is sufficient to fit the model parameters with some degree of
confidence. The time series SE of all these four parameters are less
than the posterior means (in absolute value), showing our estimates for
all of these four parameters are rather accurate and the chains was
mixing reasonably well as the trace plot or Gelman-Rubin diagnostic plot
suggested. Thus, our Bayesian Poisson GLM in part b) is accurate to
estimate the number of fish that was caught. In summary, the Bayesian
Poisson GLM in part b) is more accurate to predict the number of fish
that was caught compared to the Poisson GLM in part a).

```{r}
# Posterior sample summary
summary(model1.samps)
```

Lastly, we will check the sensitivity of our prior by specifying a new
prior for $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$. Lets try to
use the Gaussian prior with mean 0 and variance 0.15 for the intercept,
while for the other 3 regression coefficient we will use Gaussian prior
with mean 0 and variance 1000. We ran again 13 chains with 10000 samples
after 5000 burn-in iterations and printed out the posterior summaries
for the parameter $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ using
$\verb|summary(model1.coda)|$ command.

```{r}
# Input data and prior hyperparameter for each beta
fish.data <- list(n=nrow(fish),camper=fish$camper,livebait=fish$livebait,
                  prop.child=fish$prop.child,persons=fish$persons, count=fish$count,
                  mu.beta0=0,mu.beta1=0, mu.beta2=0, mu.beta3=0, prec.beta0=1, prec.beta1=1/1000,
                  prec.beta2=1/1000, prec.beta3=1/1000)

# Compile the model
model1.jags <- jags.model(file=textConnection(model1_string),data=fish.data,n.chains=13,quiet=T)

# Burnin 5000 iterations
update(model1.jags, n.iter=5000)

# Generate posterior sample
model1.coda <- coda.samples(model1.jags,n.iter=10000,variable.names=model_param)

# Posterior sample summary after change the prior
summary(model1.coda)
```

From the output above, we can see that the posterior standard deviations
are typically much smaller than the posterior means in absolute value,
indicating that the the amount of data is sufficient to fit the model
parameters with some degree of confidence. Furthermore, we can see that
all regression parameters have time series SE that is much smaller than
the posterior means (in absolute value), showing that all of our
estimates are rather accurate. It appears that as we increased the
variance parameter for the Gaussian prior of the regression coefficient
parameters, the posterior standard deviations and time series SE is
still much more smaller than the posterior means (in absolute values).

As we can see from either the $\verb|gelman.diag|$ or
$\verb|gelman.plot|$ results after we use a new prior for our parameter
of interest, the Gelman-Rubin diagnostics values are below 1.05 for each
of the regression parameters, indicating that we have approximated the
stationary distribution quite well.

```{r}
# Gelman rubin plot after change the prior
gelman.plot(model1.coda[,c('beta0','beta1')])
gelman.plot(model1.coda[,c('beta2','beta3')])

# Gelman rubin statistics after change the prior
gelman.diag(model1.coda)
```

In our code in part b), we ran 13 chains with 10000 samples after 5000
burn-in iterations but this time we use a new prior for our parameter of
interest. We have computed the ESS for the model parameters $\beta_0$,
$\beta_1$, $\beta_2$, and $\beta_3$ using the $\verb|effectiveSize|$
function. The ESS is above 1000 for each of them. It seems that the new
prior that we specified for the model parameters $\beta_0$, $\beta_1$,
$\beta_1$, $\beta_2$, and $\beta_3$ does not make the ESS below 1000.

```{r}
# Effective sample size after change the prior
effectiveSize(model1.coda)
```

Lets have a look at the trace plot of the parameter $\beta_0$,
$\beta_1$, $\beta_2$, and $\beta_3$ using the $\verb|R|$ code below
after we specified a new prior for these four parameter. From the trace
plot below, we can see that the chain for the parameter $\beta_0$,
$\beta_1$, $\beta_2$, and $\beta_3$ were mixing well and have converged.

```{r}
# Plot trace plot and density plot after change the prior
plot(model1.coda[,c('beta0','beta1')])
plot(model1.coda[,c('beta2','beta3')])
```

In summary, the new prior that we specified by increasing the value of
the variance parameter that we used in our Gaussian prior for the
coefficient parameters does not make the posterior standard deviation
and time series SE for the regression parameters $\beta_0$, $\beta_1$,
$\beta_2$, and $\beta_3$ greater than the posterior means (in absolute
values). Therefore, we can said that the amount of data is sufficient to
fit the model parameters with some degree of confidence and our
estimates for all of these four parameters are rather accurate and the
chains was mixing reasonably well even thought the values of the
variance parameter for the Gaussian prior for the coefficient parameter
is increased.

**d)[10 marks] A closer observation of the number of zeros in the
dataset reveals that they are higher than expected. A plausible
explanation for this is that some groups went to the park, but did not
go fishing at all. To better model this, we consider the zero-inflated
Poisson distribution. This is a mixture of a distribution taking 0 with
probability 1 with weight** $w$, and a Poisson distribution with weight
$1-w$ (see <https://en.wikipedia.org/wiki/Zero-inflated_model> for more
information).

**Implement the zero-inflated version of the model from part c) in JAGS.
Use a uniform prior for the weight, and the same priors for the
regression coefficients as in part c). [Hint: you can use the dbern or
dcat distributions in JAGS as part of this. In order to avoid the
incompatible error in JAGS, you may need to approximate Poisson
distribution with parameter 0 by a Poisson distribution with a small
parameter such as 0.00001.]**

**Run 5000 burn-in steps, and obtain samples from the weight parameter**
$w$ **and the regression coefficients, including the intercept). Compute
and print out the effective sample sizes (ESS) for these 5 parameters.**

**If the ESS is below 1000 for any of these 5 parameters, increase the
sample size/number of chains until the ESS is above 1000 for all of
them.**

**Print out the summary of the model. Interpret the posterior summaries
of the model parameters.**

Explanation:

In this part, we will formulate our Bayesian zero-inflated Poisson GLM
to estimates the number of fish that was caught. Assume that the number
of fish that was caught have zero-inflated Poisson distribution. Thus,
we will have
$$\text{Pr}(\text{count}=y_i)=\begin{cases}w+(1-w)e^{-\lambda}, \quad y_i = 0\\(1-w)\dfrac{\lambda^{y_i}e^{-\lambda}}{y_i!}, \quad y_i = 1,2,\dots\end{cases}$$where
$y_i$ is a non-negative integer that represents the observations for the
number of fish that was caught, $\lambda$ is the average number of fish
that was caught, and $w$ is the probability of extra zeros. The mean and
variance of the zero-inflated Poisson distribution is $w(1-\lambda)$ and
$w(1-\lambda)(1+w\lambda)$ respectively.

Now lets formulate our Bayesian zero-inflated poisson GLM. First we need
to decide what are the prior that we are going to use for the parameters
$\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$, and $w$. Recall that the
parameters $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ is the
regression parameters. We choose to used the same prior in part b) for
these four parameters. As for the parameters $w$, we will use the
uniform distribution prior $U(0,1)$ since the parameters $w$ is a
probability and the observed values that we can obtained from the
uniform distribution $U(0,1)$ will lies in the interval $[0,1]$ which is
make sense for the values of a probability. In order to avoid
incompatibility error in JAGS, we need to approximate the Poisson
distribution with parameter 0 by a Poisson distribution with smaller
parameter $\lambda$. Let $z\sim\text{Bernoulli}(w)$ and $\mu$ be the
parameter of a Poisson distribution such as $\mu\approx\lambda$ or
$\mu\approx0$. In this report, we let $\mu = \lambda z+0.00001$ to
ensure that the Poisson distribution with parameter $\mu$ have a very
close approximation to a Poisson distribution with parameter $\lambda$
or 0. Since $z\sim\text{Bernoulli}(w)$, then the possible values of $z$
is either 0 or 1. Therefore, we can have either $\mu\approx0$ or
$\mu\approx\lambda$. Let
$$\mathbf{x}_i = \left(\text{camper}_i, \text{livebait}_i, \text{prop.child}_i, \text{persons}_i\right),$$where
$\text{camper}_i$, $\text{livebait}_i$, $\text{prop.child}_i$, and
$\text{persons}_i$ each represent the i-th values of the observations
for the camper, livebait, prop.child, and persons variable respectively
for $i = 1,2,\dots,250$. Recall that there are 250 groups in the
$\verb|fish.csv|$ dataset.The Bayesian zero-inflated Poisson GLM can be
formulated as follows:$$\begin{aligned}
\text{count}_i|\mu_i, \mathbf{x}_i&\sim
\text{Poisson}(\mu_i), \quad i = 1,2,\dots, 250\\
\mu_i &= \lambda_iz_i+0.00001\\
z_i &\sim \text{Bernoulli}(w)\\
\log{(\lambda_i)} &= \beta_0+\beta_1\text{camper}_i+\beta_2\text{livebait}_i+\beta_3\text{prop.child}_i+\log{(\text{persons}_i)}\\
w &\sim U(0,1)\\
\beta_0 &\sim \text{N}(\mu_0 = 0, \sigma^2_0 = 100)\\
\beta_1 &\sim \text{N}(\mu_1 = 0, \sigma^2_1 = 1)\\
\beta_2 &\sim \text{N}(\mu_2 = 0, \sigma^2_2 = 1)\\
\beta_3 &\sim \text{N}(\mu_3 = 0, \sigma^2_3 = 1)
\end{aligned}$$

The $\verb|R|$ code below implement our Bayesian zero-inflated Poisson
GLM that we have already formulated in this part. We ran 13 chains with
10000 samples after 5000 burn-in iterations.

```{r}
# Input data and prior hyperparameter for each beta
fish.data <- list(n=nrow(fish),camper=fish$camper,livebait=fish$livebait,
                  prop.child=fish$prop.child,persons=fish$persons, count=fish$count,
                  mu.beta0=0,mu.beta1=0, mu.beta2=0, mu.beta3=0, prec.beta0=1/100, prec.beta1=1,
                  prec.beta2=1, prec.beta3=1)

# Create model block for JAGS
model2_string <- "model {
# Data that will be read in are n, count, camper, livebait, prop.child and persons

# Prior for each parameter beta and the weight (w)
beta0 ~ dnorm(mu.beta0,prec.beta0)
beta1 ~ dnorm(mu.beta1,prec.beta1)
beta2 ~ dnorm(mu.beta2,prec.beta2)
beta3 ~ dnorm(mu.beta3,prec.beta3)
w ~ dunif(0,1)

#Likelihood
for(i in 1:n) {

# Poisson GLM
log(lambda[i]) <- beta0+beta1*camper[i]+beta2*livebait[i] +beta3*prop.child[i]+log(persons[i])

#Zero-inflated Poisson model
z[i] ~ dbern(w)
mu[i] <- lambda[i]*z[i] + 0.00001

# Observations
count[i] ~ dpois(mu[i])

# Replicates
count.rep[i] ~ dpois(mu[i])}
}"

# Compile the model
model2 <- jags.model(file=textConnection(model2_string),data=fish.data,n.chains=13,quiet = T)

# Burnin 5000 iterations
update(model2, n.iter=5000)

# Generate posterior sample
model_param <- c("beta0","beta1","beta2", "beta3", "w")
model2.samps <- coda.samples(model2,variable.names=model_param,n.iter=10000)
```

As we can see from either the $\verb|gelman.diag|$ or
$\verb|gelman.plot|$ results, the Gelman-Rubin diagnostics values are
below 1.05 for each of the parameters of interest in our posterior
sample, indicating that we have approximated the stationary distribution
quite well.

```{r}
# Gelman rubin plot
gelman.plot(model2.samps[,c('beta0','beta1')])
gelman.plot(model2.samps[,c('beta2','beta3')])
gelman.plot(model2.samps[,c('w')], main = 'w')

# Gelman rubin statistics
gelman.diag(model2.samps)
```

In our zero-inflated Poisson JAGS code attached in this part, we ran 13
chains with 10000 samples after 5000 burn-in iterations. We have
computed the ESS (Effective Sample Size) for the model parameters $w$,
$\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ using the
$\verb|effectiveSize|$ function. The ESS is above 1000 for each of them,
so no further samples are needed.

```{r}
# get effective sample size
effectiveSize(model2.samps)
```

Lets have a look at the trace plot of the regression parameter $w$,
$\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ using the $\verb|R|$ code
below. From the trace plot below, we can see that the chain for the
parameter $w$, $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ were
mixing well and have converged.

```{r}
# Plot trace plot and density plot
par(mar=c(3,3,3,3))
plot(model2.samps[,c('w','beta0','beta1')])
plot(model2.samps[,c('beta2','beta3')])
```

Next, we printed out the posterior summaries for the parameters $w$,
$\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ using the
$\verb|summary(model2.samps)|$ command. We can see that the posterior
standard deviations for these five parameters are typically much smaller
that the posterior means in absolute value, indicating that the amount
of data is sufficient to fit the model parameters with some degree of
confidence. The time series SE of all these five parameters are less
than the posterior means (in absolute value), showing our estimates for
all of these five parameters are rather accurate and the chains was
mixing reasonably well as the trace plot or Gelman-Rubin diagnostic plot
suggested.

```{r}
# Posterior sample summary
summary(model2.samps)
```

**e)[10 marks]**

**Compare the quality of the model fit for the two models via DIC
scores.**

**Perform posterior predictive checks for both models for 3 functions:**

**1. number of groups catching no fish,**

**2. number of groups catching more than 5 fish,**

**3. average number of fish caught among groups who brought a camper
van.**

**Discuss your results.**

Explanation:

First, we will perform posterior predictive checks on the Bayesian
Poisson GLM model in part b). The histograms below show posterior
predictive distribution for the average number of fish caught among
groups who brought a camper van (right plot on first row), number of
groups catching no fish (left plot on first row), and number of groups
catching more than 5 fish (right plot on second row) for the Bayesian
fit to the number of fish that was caught. The descriptive statistic
values (the average number of fish caught among groups who brought a
camper van, number of groups catching no fish, and number of groups
catching more than 5 fish) in the observed dataset are shown by vertical
red lines. The lines seem to be within the typical range of the
replicates for the posterior predictive distribution of the average
number of fish that was caught among groups who brought a camper van,
while in the case of the posterior predictive distributions of the
number of groups catching no fish and the number of groups catching more
than 5 fish are lies outside the typical ranges of the replicates. This
indicates that there is an issues with the priors that was used Poisson
GLM in part b) that have been detected by this posterior predictive
checks when predicting the posterior predictive distributions for the
number of groups catching no fish and the number of groups catching more
than 5 fish .

```{r, message = FALSE, warning = FALSE}
library(fBasics)

# Generate replicate observations
res1.replicates <- coda.samples(model1,variable.names=c("count.rep"),n.iter=10000,progress.bar="none")

countrep <- as.matrix(res1.replicates)

# Compute the required data for posterior predictive checks
ind.camper <- which(fish$camper==1)
ind.count0 <- which(fish$count==0)
ind.count5.more <- which(fish$count > 5)

count.camper <- fish[ind.camper,'count']
countrep.camper <- countrep[,ind.camper]

countrep.camper.mean <- apply(countrep.camper,1,mean)

count0 <- fish[ind.count0,'count']
countrep0 <- countrep[,ind.count0]

countrep0.count <- rowSums(countrep==0)

countrep5.great.count <- rowSums(countrep>5)

m1 <- max(countrep.camper.mean,mean(fish$count[ind.camper]))
m2 <- max(countrep0.count,length(ind.count0))
m3 <- max(countrep5.great.count,length(ind.count5.more))

# Plot the posterior predictive distribution histogram
par(mfrow=c(2,2))
hist(countrep.camper.mean,col="gray40",xlim=c(0,m1),cex.main = 0.75,
     main='Predictive Distribution Capture Fish (Camper)')
abline(v = mean(fish$count[ind.camper]),col='red',lwd=2)

hist(countrep0.count,col="gray40",xlim=c(0,m2),cex.main = 0.75,
     main='Predictive Distribution Groups Capture No Fish')
abline(v = length(ind.count0),col='red',lwd=2)

hist(countrep5.great.count,col="gray40",xlim=c(0,m3),cex.main = 0.75,
     main='Predictive Distribution Groups Capture >5 Fish')
abline(v = length(ind.count5.more),col='red',lwd=2)
```

Next, we will perform posterior predictive checks on the Bayesian
zero-inflated Poisson GLM model in part d). The histograms below show
posterior predictive distribution for the average number of fish caught
among groups who brought a camper van (right plot on first row), number
of groups catching no fish (left plot on first row), and number of
groups catching more than 5 fish (right plot on second row) for the
Bayesian fit to the number of fish that was caught. The descriptive
statistic values (the average number of fish caught among groups who
brought a camper van, number of groups catching no fish, and number of
groups catching more than 5 fish) in the observed dataset are shown by
vertical red lines. The lines seem to be within the typical range of the
replicates for the posterior predictive distribution of the average
number of fish that was caught among groups who brought a camper van and
the posterior predictive distributions of the number of groups catching
no fish, while in the case of the posterior predictive distributions of
the number of groups catching more than 5 fish are lies outside the
typical ranges of the replicates. This indicates that there is an issues
with the priors that was used in the zero-inflated Poisson GLM in part
d) that have been detected by this posterior predictive checks when
predicting the posterior predictive distributions for the number of
groups catching more than 5 fish.

```{r}
# Generate replicate samples
res2.replicates <- coda.samples(model2,variable.names=c("count.rep"), n.iter=10000,progress.bar="none")
countrep <- as.matrix(res2.replicates)

# Compute required data for posterior predictive checks
ind.camper <- which(fish$camper==1)
ind.count0 <- which(fish$count==0)
ind.count5.more <- which(fish$count > 5)

count.camper <- fish[ind.camper,'count']
countrep.camper <- countrep[,ind.camper]

countrep.camper.mean <- apply(countrep.camper,1,mean)

count0 <- fish[ind.count0,'count']
countrep0 <- countrep[,ind.count0]

countrep0.count <- rowSums(countrep==0)

countrep5.great.count <- rowSums(countrep>5)

m1 <- max(countrep.camper.mean,mean(fish$count[ind.camper]))
m2 <- max(countrep0.count,length(ind.count0))
m3 <- max(countrep5.great.count,length(ind.count5.more))

# Plot the posterior predictive probability histogram
par(mfrow=c(2,2))
hist(countrep.camper.mean,col="gray40",xlim=c(0,m1),cex.main = 0.75,
     main='Predictive Distribution Capture Fish (Camper)')
abline(v = mean(fish$count[ind.camper]),col='red',lwd=2)

hist(countrep0.count,col="gray40",xlim=c(0,m2),cex.main = 0.75,
     main='Predictive Distribution Groups Capture No Fish')
abline(v = length(ind.count0),col='red',lwd=2)

hist(countrep5.great.count,col="gray40",xlim=c(0,m3),cex.main = 0.75,
     main='Predictive Distribution Groups Capture >5 Fish')
abline(v = length(ind.count5.more),col='red',lwd=2)
```

Lastly, we will compute the DIC score for the Bayesian Poisson GLM in
part b) and the Bayesian zero-inflated Poisson GLM in part d) using the
function $\verb|dic.samples()|$ in the $\verb|R|$ code below.

```{r}
# DIC score for original Poisson GLM
dic.samples(model1, n.iter=10000)
```

```{r}
# DIC score for Zero-Inflated Poisson GLM
dic.samples(model2, n.iter=10000)
```

From the output above, we can see that the DIC score for the Bayesian
zero-inflated Poisson GLM in part d) is lower than the DIC score for the
Poisson GLM in part b), which indicates that the Bayesian zero-inflated
Poisson GLM in part d) is fits better on the data compared to the
Bayesian Poisson GLM in part b).

![](money.jpg){width="100%"}

**Problem 2 - Personal loans data**

**In this problem, we are going to analyze some personal loans data from
a US bank named LendingClub.**

**The dataset contains information about 188181 personal loans in the US
whose term has already completed.**

**The column that we aim to model is loan.status, which is either
''Fully Paid", or "Charged Off". In our response variable** $y$**, value
1 will to "Charged Off", while value 0 corresponds to "Fully paid".
Hence we are modelling whether or not the client defaulted on their
loan.**

**The other columns in the dataset are**

**loan.amnt: The amount of principle given in the loan in USD**

**interest.rate: The interest rate assigned to the loan by LendingClub**

**sub.grade: A grade assigned to the loan internally by LendingClub**

**annual.income: The lendee's annual income in USD**

**debt.to.income: The lendee's debt-to-income ratio. This explains what
percent of the client's income is spent on loans each month before
taking out this new personal loan (such as mortgage, rent, car loan,
etc., see
<https://www.lendingclub.com/loans/resource-center/calculating-debt-to-income>)**

**state, the US state in which the borrower lives (in a 2 letter
abbreviated format)**

**term, the term of the loan, i.e. the number of months in which it has
to be paid off**

**loan.status: The final status, either "Paid" or "Charged Off", of the
loan**

**We are going to use INLA to fit several different logistic regression
models to this dataset. First, we load ILNA and the dataset and display
the first few rows.**

```{r, warning = FALSE, message = FALSE}
rm(list=ls()) # Empty memory used in question 1
library(INLA)

#If it loaded correctly, you should see this in the output:
#Loading required package: Matrix
#Loading required package: foreach
#Loading required package: parallel
#Loading required package: sp
#This is INLA_21.11.22 built 2021-11-21 16:13:28 UTC.
# - See www.r-inla.org/contact-us for how to get help.
# - To enable PARDISO sparse library; see inla.pardiso()

#The following code does the full installation. You can try it if INLA has not been installed.
#First installing some of the dependencies
#install.packages("BiocManager")
#BiocManager::install("Rgraphviz")
#if (!requireNamespace("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
#BiocManager::install("graph")
#Installing INLA
#install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
#library(INLA)
```

```{r, warning = FALSE, message = FALSE}
lending <- read.csv(file = 'lending.csv')
head(lending)
```

**a)[10 marks]**

**In this question, we are going to fit a logistic GLM on the dataset.
First, we need to do some data manipulation.**

**Create a response column y such that** $y=1$ **corresponds to
loan.status being "Charged Off", and** $y=0$ **corresponds to
loan.status being "Paid".**

**Convert the interest.rate and debt.to.income columns from strings into
numbers [Hint: you can use the str_replace_all function from the stringr
library to get rid of the % signs].**

**Create new columns for the logarithm of the loan amount, the logarithm
of the interest rate, the logarithm of the annual income.**

**Scale these 3 columns (i.e. center them and divide them by their
standard deviation). Also scale the debt-to-income column (but do not
apply log transformation).**

**Fit a logistic GLM on the response y as a function of the scaled log
interest rate, scaled log annual income, scaled log loan amount, scaled
debt-to-income, sub.grade, and term (sub.grade and term can be kept
categorical). Interpret the results.**

Explanation:

Before we begin analyze our data, we will start by introducing the
terminology of a defaulted loans and charge-off that we are going to use
in this report. Default is the failure to repay a debt, including
interest or principal, on a loan or security. A default can occur when a
borrower is unable to make timely payments, misses payments, or avoids
or stops making payments. Individuals, businesses, and even countries
can default if they cannot keep up their debt obligations. The reference
that I use to explain the terminology defaulted loan is cited from this
following link: <https://www.investopedia.com/terms/d/default2.asp>.

Meanwhile, charge-off refers to debt that a company believes it will no
longer collect as the borrower has become delinquent on payments.
Charged-off debt does not mean that the consumer does not have to repay
the debt anymore. After a lender has charged off a debt, it could sell
the debt to a third-party collections agency that would attempt to
collect on the delinquent account. A consumer owes the debt until it is
paid off, settled, discharged in a bankruptcy proceeding, or in case of
legal proceedings, becomes too old due to the statute of limitations.
The reference that I used to explained the charge-off terminology is
cited from this following link:
[https://www.investopedia.com/terms/c/chargeoff.asp.](https://www.investopedia.com/terms/c/chargeoff.asp#:~:text=A%20charge%2Doff%20refers%20to,to%20repay%20the%20debt%20anymore.)

In this part, we will fit a logistic GLM with the binary response
variable is the loan status and the explanatory variables are the
logarithm of interest rate, logarithm of annual income, logarithm of
loan amount, debt-to-income ratio, grade assigned to the loan, and the
term of the loan. Before we fit the data of the response and explanatory
variables, we will do some data manipulation. First, we will convert the
loan status variable into a binary variable by converting the loan
status "Charged Off" into 1 and the loan status "Fully Paid" into 0.
Next, we will convert the interest rate and debt-to-income ratio by
removing the "%" pattern from the data using $\verb|str_replace_all()|$
function in $\verb|R|$ and then convert the variable into a numeric
variable by using the $\verb|as.numeric()|$ function in $\verb|R|$.
After that, we calculate the logarithm of the loan amount, the logarithm
of the interest rate, and the logarithm of the annual income using the
$\verb|log()|$ function in $\verb|R|$. We add this three variable to the
data and scale the data (centering the data by the means then divide by
the standard deviations) using the $\verb|scale()|$ function in
$\verb|R|$. In addition, we will also used the $\verb|scale()|$ function
on the debt-to-income data but we would not use log transformations on
these data. Lastly, we will fit our logistic GLM model using the
$\verb|R|$ code below.

```{r}
library(stringr)

# Create response column y
lending$y <- ifelse(lending$loan.status=="Charged Off",1,0)

# Extract numbers from interest rate and debt-to-income
lending$interest.rate <- as.numeric(str_replace_all(lending$interest.rate,"%", ""))
lending$debt.to.income <- as.numeric(str_replace_all(lending$debt.to.income,"%", ""))

# Create logarithm on loan amount, interest rate, and annual income
lending$log.loan.amount <- log(lending$loan.amount)
lending$log.interest.rate <- log(lending$interest.rate)
lending$log.annual.income <- log(lending$annual.income)

# Scale the converted log column and debt-to-income
lending$log.loan.amount <- scale(lending$log.loan.amount)
lending$log.interest.rate <- scale(lending$log.interest.rate)
lending$log.annual.income <- scale(lending$log.annual.income)
lending$debt.to.income <- scale(lending$debt.to.income)

# Logistic GLM model
m1.logit <- glm(y~log.interest.rate+log.annual.income+log.loan.amount+debt.to.income+sub.grade+term,
                data = lending, family = binomial)
summary(m1.logit)
```

We display our fitted logistic GLM summary statistics using the
$\verb|summary(m1.logit)|$ command. We obtained our estimates for the
intercept parameter is -3.291996. Thus, we can said that the probability
of the loan status is defaulted by the clients is exp(-3.291996)
(approximately 3.72%) times the probability of the loan status is not
defaulted by the clients when the values of all the numerical variable
observations is zero and the observations of all categorical variable
are at the reference category. Therefore, we can said that the
probability of the loan status is defaulted by the clients is lower than
the the probability of the loan status is not defaulted by the clients
under the assumptions when the values of all the numerical variable
observations is zero and the observations of all categorical variable
are at the reference category. Now lets interpret the other estimated
parameter for the coefficient of the numerical variable in our logistic
GLM model.

We can see that the estimated coefficient parameters for the logarithm
of the interest rate is 0.011836. This implies that the probability of
the loan status is defaulted by the clients is exp(0.011836)
(approximately 1.01) times the probability of the loan status is not
defaulted by the clients for every increase the value of the logarithm
of interest rate by 1 unit assuming all other explanatory variables
remain constant.

Now lets interpret the estimated coefficient parameters on the logarithm
of annual income variables. We can see that the estimated coefficient
parameters for the logarithm of the annual income is -0.283867. This
implies that the probability of the loan status is defaulted by the
clients is exp(-0.283867) (approximately 0.75) times the probability of
the loan status is not defaulted by the clients for every increase the
value of the logarithm of annual income by 1 unit assuming all other
explanatory variables remain constant.

Next, lets interpret the estimated coefficient parameters on the
logarithm of loan amount variables. We can see that the estimated
coefficient parameters for the logarithm of the loan amount is 0.145856.
This implies that the probability of the loan status is defaulted by the
clients is exp(0.145856) (approximately 1.16) times the probability of
the loan status is not defaulted by the clients for every increase the
value of the logarithm of loan amount by 1 unit assuming all other
explanatory variables remain constant.

Lastly lets interpret the estimated coefficient parameters on the
debt-to-income variables. We can see that the estimated coefficient
parameters for the debt-to-income is 0.078952. This implies that the
probability of the loan status is defaulted by the clients is
exp(0.078952) (approximately 1.08) times the probability of the status
is not defaulted by the clients for every increase the value of the
debt-to-income by 1 unit assuming all other explanatory variables remain
constant.

Recall that the loans are classified by grade (A through G) and
sub-grade (1 through 5). As for the estimated coefficient for the dummy
variable of the sub-grade categorical variable, we can see that as the
grade increased from A (lowest risk) to G (highest risk), the estimated
coefficient of each sub-grade in each grade increased. As the estimated
coefficient increased, then the risk of a loan will be defaulted becomes
higher than the loan will be un-defaulted. Therefore, we can said that
loan in the grade G will have higher probability of being defaulted,
while loan from grade A will have lower probability of being defaulted.
The description about the loan grade can be find in this following link:
<https://www.marlo.online/loan-grades>.

As for the terms variable, we can see that when the loan terms is 60
months, then the probability of the loan being defaulted will be
exp(0.339471) (approximately 1.404) times the probability of a loan
being un-defaulted when other explanatory variables remained constant.

The exponential values of the regression parameters that we are using
here is provided by the $\verb|R|$ code below.

```{r}
#exp(beta)where beta is regression parameter
exp(coef(m1.logit))
```

We can evaluate our logistic GLM curve performance using the AUC score.
AUC represents the degree or measure of separability. It tells us how
much the model is capable of distinguishing categorical values between
classes (category in the response categorical variable). If the AUC
become higher, our model become much more better at predicting the
categorical variable values. By analogy, the higher the AUC, the better
the logistic GLM is at distinguishing between clients with the loan
status charged off and fully paid.

The area under the ROC curve (AUC) results were considered excellent for
AUC values between 0.9-1, good for AUC values between 0.8-0.9, fair for
AUC values between 0.7-0.8, poor for AUC values between 0.6-0.7 and
failed (very poor) for AUC values between 0.5-0.6.

In the ROC curves, we can see that the AUC score for the logistic GLM is
between 0.6 and 0.8, which suggest that our model is performs poor at
distinguishing between clients with the loan status charged off and
fully paid. Further references about an interpretation of AUC score can
be found in this following link:
[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2935260/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2935260/#:~:text=The%20area%20under%20the%20ROC,AUC%20values%20between%200.5%2D0.6.).

There are two metrics observed in the ROC curves: sensitivity and
specificity. Sensitivity (true positive rate) is the metric that
evaluates a prediction model's ability to predict true positives (in our
case is charged off loan status) of each available category in our
binary response variable (loan status). On the other hands, specificity
(true negative rate) is the metric that evaluates a prediction model's
ability to predict true negatives (in our case is fully paid loan
status) of each available category in our binary response variable (loan
status). Further reference about specificity and sensitivity can be
found in this following link:
<https://en.wikipedia.org/wiki/Sensitivity_and_specificity>

The $\verb|R|$ code below computes the AUC score and plot the ROC
curves.

```{r, warning=FALSE, message=FALSE}
library(pROC)
#plot ROC curve for the logistic GLM model
prob <- predict(m1.logit, newdata = lending, type = "response")
roc(loan.status ~ prob, data = lending, plot = T, print.auc = T, quiet = T, 
    main= 'Model 1 ROC Curve')
```

From to the ROC curve above we can see that as the performance of our
logistic GLM decreased in terms of identifying a client who have fully
paid loan status is decreased, the performance of our model in terms of
identifying the client who have charged off loan status is increased. In
the opposite, if the performance of our logistic GLM decreased in terms
of identifying a client who have charged off loan status is decreased,
the performance of our model in terms of identifying the client who have
fully paid loan status is increased.

**b)[10 marks] (Identical Model)**

**Implement a Bayesian logistic GLM of the same structure as in a) in
INLA [Hint: use need to use the "binomial" family with Ntrials=1 to
indicate logistic regression in INLA, see
<https://rpubs.com/corey_sparks/431920> for an example).**

**Propose your own priors for the model parameters (you can take into
account slides 45-46 of Lecture 4).**

**Print out the model summary. Check the sensitivity of the results with
respect to the prior choices.**

**Compute DIC and negative log-CPO (NLSCPO).**

**Do posterior predictive checks with the functions being evaluated
chosen as the proportion of defaults among clients in each of the 49
states contained in the dataset [Hint: plot these in a 7 x 7 format
using par(mfrow=c(7,7)). Make sure to write par(mfrow=c(1,1)) after your
plots to change back to the default setting]. Discuss the model fit.**

Explanation:

In this part, we are going to implement the identical model, which in
this case is a Bayesian logistic GLM requiring only fixed effects. The
model used the same intercept for all the 49 states. First, we are going
to choose the prior for our regression parameters. Observe that in USA
there are six types of debt: personal loan, student loan, auto loan,
credit card, HELOC (Home Equity Lines of Credit), and mortgage. Overall,
mortgages by far represent the largest outstanding debt in the USA. The
average mortgage balance stands at \$208,185, and 44% of USA adults have
this type of debt.

As for the personal loans, nearly 25% of USA adults have this type of
debt, and personal loan average American debt stands at \$16,458. The
percentage of accounts that were 30 or more days past due decreased by
27% between 2019 and 2020.

The average debt of personal loans in 2020 is \$16,458, which is
significantly smaller than the average debt of mortgages in 2020 (which
is \$208,185). Compare to other consumer debts, personal loans continue
to make up the smallest sliver of consumer debt held by Americans
(around 0.9% of the total debt consumer in America), despite their
growth over the past decade.

The reference about debt in USA is provided in this following links:
[https://www.bankrate.com/personal-finance/debt/average-american-debt/](https://www.bankrate.com/personal-finance/debt/average-american-debt/#:~:text=Personal%20loan%20debt&text=Nearly%20a%20quarter%20of%20U.S.,percent%20between%202019%20and%202020.).
Furthermore, further reference about personal loans can be seen in this
following links:
<https://www.lendingtree.com/personal/personal-loans-statistics/>.

Since most American mostly have smaller debt of personal loans despite
the increased of the outstanding balance of personal loans, then it
appears that the probability of un-defaulted personal loans is higher
than the probability of defaulted personal loans. Therefore, we choose
to use the Gaussian prior $\text{N}\left(0,100\right)$ for all the
regression parameter in the logistic GLM to ensure that the logarithm of
defaulted probability per un-defaulted probability less than zero (which
equivalently to ensure that the probability of a loan status being
charge-off is less than the probability of a loan status being
fully-paid). As we can see from the density plot below, the normal
distribution $\text{N}\left(0,100\right)$ is more centered around the
mean ($\mu = 0$) and the log-normal distribution
$\log\text{N}\left(0,100\right)$ tends to decreased in order to
approached zero. Thus, choosing the Gaussian prior for the regression
parameter to ensure that the probability of a loan status being
charge-off is less than the probability of a loan status being
fully-paid seems quite make sense.

```{r}
# Plot N(0,100) and logN(0,100) density curve
xx <- -100:100
plot(xx,dnorm(xx,0,10),col='red',type='l',xlab="",ylab="")
lines(xx,dlnorm(exp(xx),0,10),col='blue')
legend("topright", c(expression(N(0,10^2)),expression(logN(0,10^2))),
       col = c("red","blue"),lwd = 1,bty='n',cex=0.7)
```

Since the variance parameter of our Gaussian prior is $\sigma^2 = 100$,
then the precision parameter for our Gaussian prior is
$\tau = \frac{1}{\sigma^2} = 0.01$. The $\verb|R|$ code below implement
our identical model with these Gaussian prior for all of the regression
parameters and printed out the model summary using the
$\verb|summary(m2.I)|$ command. We set $\verb|Ntrials|=1$,
$\verb|family| = \verb|binomial|$, and $\verb|control.family|$ equals to
$\verb|list(link="logit")|$ in order to use logistic GLM in in INLA. In
this part, we will only use the first 90000 observations to build our
identical model since INLA will crash and produce error if we use all
the observations in our dataset.

```{r}
# Prior for regression parameters
prior.beta <- list(mean.intercept = 0,prec.intercept = 0.01,mean = 0,prec = 0.01)

# Fit the logistc GLM identical model into INLA
m2.I<-inla(y~log.interest.rate+log.annual.income+log.loan.amount+debt.to.income+sub.grade+term,
           data=lending[1:90000,],family="binomial",Ntrials = 1,
           control.family=list(link="logit"),control.fixed=prior.beta,
           control.compute=list(waic=T,config=T,cpo=T,dic=T))

# Display model summary
summary(m2.I)
```

As we can see from the model summary output above, the column
$\verb|0.025quant|$ and $\verb|0.975quant|$ represents the lower and
upper bound of a 95% credible interval for our regression parameter. In
Bayesian statistics, we can interpret the 95% credible interval for the
regression parameter as a fix interval where the value of interest of
our regression parameter will lies on that interval with probability
95%.

If we observe the output of the identical model that we obtained from
INLA using the $\verb|summary(m2.I)|$ command, we can see that the
posterior mean for our identical model parameter are not significantly
different from the estimated values of our logistic GLM parameter in a).
This implies that the our estimated regression parameter of the
identical model that we obtained using INLA by specifying our proposed
Gaussian prior $\text{N}(0,100)$ does not significantly change the
estimated regression parameter even though we use different method than
in part a) to obtain the estimated values of our parameter of interest.
Furthermore, we can see that the values of the posterior mean and
standard deviation for the posterior distribution of the regression
parameter coefficient for the explanatory variable that we obtained
using the $\verb|summary(m2.I)|$ command is significantly different from
zero, which implies that all the explanatory variable that we used in
the identical model is most likely important in the model.

Lets try to see the model summary output from INLA using different
Gaussian prior. We test different Gaussian prior to check the
sensitivity of the estimated logistic GLM parameter posterior mean in
order to investigate does different Gaussian prior affect the estimated
values of our regression parameters.

```{r}

prior.check <- function(mu.beta0,prec.beta0,mu.beta,prec.beta,dat){
  # Function to display summary statistics for different prior
  #Input:
  #mu.beta0 = mean parameter in Gaussian prior for beta_0 
  #mu.beta = mean parameter in Gaussian prior for beta_1,..,beta_n
  #prec.beta0 = precision parameter in Gaussian prior for beta_0 
  #prec.beta0 = precision parameter in Gaussian prior for beta_1,..,beta_n
  #dat = inputed data
  #Output: model summary from INLA
  
  # Prior for regression parameter
  prior.beta <- list(mean.intercept = mu.beta0,prec.intercept = prec.beta0,
                     mean = mu.beta,prec = prec.beta)
  
  # Fit the logistc GLM identical model into INLA
  m.I<-inla(y~log.interest.rate+log.annual.income+log.loan.amount+debt.to.income+sub.grade+term,
            data=dat,family="binomial",Ntrials = 1,
            control.family=list(link="logit"),control.fixed=prior.beta,
            control.compute=list(waic=T,config=T,cpo=T,dic=T))
  
  # Display model summary
  summary(m.I)
}

#Perform first prior sensitivity check
prior.check(mu.beta0=0,prec.beta0=1e-6,mu.beta=0,prec.beta=1e-6,dat=lending[1:90000,])
```

```{r}
# Perform second prior sensitivity check
prior.check(mu.beta0=0,prec.beta0=100,mu.beta=0,prec.beta=100,dat=lending[1:90000,])
```

We can see that the regression parameter of estimates (in this case the
posterior mean in the $\verb|mean|$ column) is not significantly
different from the regression parameter of estimates using our Gaussian
original prior under the case we reduces the values of the precision
parameter. However, as we increased the values of the precision
parameter in our Gaussian prior, it turns out that the result showed a
great difference in terms of the estimated parameters. Thus, we can said
that the Gaussian prior we used have a tendency to produces estimated
values of regression parameter which is closer to the estimated values
like in the logistic GLM part a) if we used smaller precision.

Now lets compute the DIC and negative log-CPO (NSLCPO). According to the
output below, we can see that the NSLCPO and DIC are 36154.35 and
72308.57 respectively.

```{r}
# Compute NSLCPO
cat("NSLCPO model 2:",-sum(log(m2.I$cpo$cpo)),"\n")

# Compute DIC
cat("DIC model 2:",m2.I$dic$dic,"\n")
```

Lastly, we will perform posterior predictive checks on the posterior
predictive distribution of the proportion of defaults among clients in
each of the 49 states contained in the dataset. As the Predictor
variables in the posterior samples of INLA contain the linear predictors
$\eta_i = \text{logit}(p_i)$ where
$\eta_i=\beta_0+\sum\limits_{j=1}^{n}\beta_{j}x_{ji}$, in order to get
the posterior predictive samples of the proportion of defaults among
clients in each of the 49 states, we need to apply the inverse logit
function on these linear predictors. The logit link function is of the
form $$\text{logit}(p)=\log{\left(\dfrac{p}{1-p}\right)},$$and so the
inverse logit function is
$$\text{inv.logit}(x) = \dfrac{1}{1+e^{-x}}.$$The $\verb|R|$ code below
perform a posterior predictive check on the posterior predictive
distribution of the proportion of defaults among clients in each of the
49 states.

```{r, warning = FALSE, message = FALSE}
library(fBasics)

# Create inverse logistic function
inv.logit <- function(x) {
  return(1/(1+exp(-x)))
}

# Generate replicate samples
nbsamp=100
n=nrow(lending[1:90000,])

lending.identical.samples=inla.posterior.sample(n=nbsamp, result=m2.I)

predictor.samples.identical=inla.posterior.sample.eval(function(...) {Predictor},
                                                       lending.identical.samples)


# Calculate replicate probability defaulted loans
probarep1 <- rowMeans(inv.logit(predictor.samples.identical))

# Plot histogram of predictive distribution for each state

par(mfrow=c(7,7),mar=c(1,1,1,1))

for (i in unique(lending$state[1:n])){
  ind.default <- which(lending$state[1:n]==i & lending$y[1:n]==1)
  ind.state <- which(lending$state[1:n]==i)
  proba.default <- length(ind.default)/length(ind.state)
  max.vert <- max(c(proba.default,probarep1[ind.state]))
    min.vert <- min(c(proba.default,probarep1[ind.state]))
    hist(probarep1[ind.state],col="gray40",xlim=c(min.vert,max.vert),cex.main = 0.7,
         cex.axis=0.5,main=i)
  abline(v = proba.default,col='red',lwd=2)
}
```

The histograms above show posterior predictive distribution for the
proportion of defaults among clients in each of the 49 states for the
Bayesian fit to the loan status in the $\verb|lending.csv|$ dataset. The
proportion of defaults among clients in each of the 49 states in the
observed dataset are shown by vertical red lines.

Based on these plots, we can confirm that for several state (NE, IA, and
MS), the observed number of proportion of defaults among clients falls
outside the range of the replicates proportion of defaults in the
predictive distributions plot. This might be due to we only use the
first 90000 observations in the dataset instead using all observations
in the dataset.

**c)[10 marks] (Independent model)**

**Using INLA, implement a Bayesian logistic GLM with y as a function of
the scaled log interest rate, scaled log annual income, scaled log loan
amount, scaled debt-to-income, and sub.grade, and an independent
intercept for the state variable (see the Radon levels example on page
38 of Lecture 5).**

**Propose your own priors for the model parameters (you can take into
account slides 57-58 of Lecture 4).**

**Print out the model summary.**

**Compute DIC and negative log-CPO (NLSCPO).**

**Do posterior predictive checks with the functions being evaluated
chosen as the proportion of defaults among clients in each of the 49
states contained in the dataset. Discuss the model fit.**

Explanation:

In this part, we are going to implement the independent model, which in
this case is a Bayesian logistic GLM where each state has an independent
random intercept. This can be achieved using random effects in INLA by
adding $\verb|0|$ and
$$\verb|f(state,model="iid",hyper=prec.prior.random.eff)|$$into the
model formula that we are provide to INLA in our $\verb|R|$ code, with
the appropriate prior distribution set by the $\verb|hyper|$ attribute.
Note that there is no "global" intercept in the independent model, hence
we specified it to be $\verb|0|$ in the formula. The reference that we
used for understanding the logistic GLM independent model was inspired
from this following link:
[https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html](https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#sec:binarydata).

Now, we will specify the prior that we are going to use in INLA. For the
regression parameters, we will use the same Gaussian prior in part b).
Thus the Gaussian prior for the regression parameters in our independent
model is $\text{N}\left(0,100\right)$.

Next, we will specify the prior for the random effect terms $u_j$, where
$j = 1,2,\dots,49$. We will used the Gaussian prior with mean 0 and
variance $\sigma^2_u$ for the random effect terms $u_j$. In addition, we
choose the prior distribution for $\sigma_u$ be a Uniform distribution
$U[0,10]$ to ensure that the standard deviation of $\sigma_u$ becomes
much more larger, which will ensure that the logarithm of defaulted
probability per un-defaulted probability less than zero (which
equivalently to ensure that the probability of a loan status being
charge-off is less than the probability of a loan status being
fully-paid). As in INLA, we need to specify the prior on
$\log{(\tau_u)}$ for $\tau_u = \frac{1}{\sigma^2_u}$ where $\tau_u$ is
the precision parameter of the Gaussian prior for the random effect
terms $u_j$. As explained in Section 5.3.2 of Bayesian Inference in INLA
(<https://becarioprecario.bitbucket.io/inla-gitbook/ch-priors.html>),
the corresponding prior on $\theta=\log{(\tau_u)}$ can be written as
$$\log{(\pi(\theta))} = \log{\left(\pi_{\sigma_u}\left(e^{-\frac{\theta}{2}}\right)\right)}-\frac{\theta}{2}-\log{(2)}.$$In
the specific case of $\sigma_u \sim U[0,b]$, we have
$\pi_{\sigma_u}(x) = \frac{1}{b}$ for $x \in [0,b]$, and 0 elsewhere. So
after rearrangement, it follows that
$$\log{(\pi(\theta))} = \begin{cases}-\log{(b)}-\frac{\theta}{2}-\log{(2)} \quad \text{if} \quad \theta \geq -2\log{(b)},\\ -\infty \quad \text{if} \quad \theta < -2\log{(b)}.\end{cases}$$We
can input this prior in INLA using the "expression:" string, see the
$\verb|R|$ code below that also implement the independent model. In this
part, we choose to use the first 90000 first observations in the
$\verb|lending.csv|$ dataset to ensure that INLA would not crashed and
also not produced error when we fit too much dataset.

```{r}
sigma.unif.prior.random.eff = "expression:
  b = 10;
  log_dens = (theta>=(-2*log(b)))*(-log(b)-theta/2-log(2)) + (theta<(-2*log(b)))*(-Inf);
  return(log_dens);"

b=10;

#We select the prior for the regression coefficients
prior.beta <- list(mean.intercept = 0,prec.intercept = 0.01,mean = 0, prec = 0.01)

#The hyperparameter precision of the random effect is stored on the log-scale, 
#and it has to be input in this form when specifying it
#fixed=TRUE ensures that it is fixed at its initial value
prec.prior.random.eff <- list(prec=list(prior=sigma.unif.prior.random.eff,
                                        initial=(-2*log(b)+1),fixed = FALSE))


# Fit the logistic GLM independent model in INLA
m3.I<-inla(y~0+log.interest.rate+log.annual.income+log.loan.amount+debt.to.income+sub.grade
           +term+f(state,model="iid",hyper=prec.prior.random.eff),
           data=lending[1:90000,],family ="binomial",Ntrials=1,
           control.family=list(link="logit"),control.fixed=prior.beta,
           control.compute=list(waic=T,config=T,cpo=T,dic=T))

# Display model summary
summary(m3.I)
```

As we can see from the model summary output above, the column
$\verb|0.025quant|$ and $\verb|0.975quant|$ represents the lower and
upper bound of a 95% credible interval for our regression parameter. In
Bayesian statistics, we can interpret the 95% credible interval for the
regression parameter as a fix interval where the value of interest of
our regression parameter will lies on that interval with probability
95%.

If we observe the output of the independent model that we obtained from
INLA using the $\verb|summary(m3.I)|$ command, we can see that the
values of the posterior mean and standard deviation for the posterior
distribution of the regression parameter coefficient for the explanatory
variable that we obtained using the $\verb|summary(m3.I)|$ command is
significantly different from zero, which implies that all the
explanatory variable that we used in the independent model is most
likely important in the model.

Next, we printed out the summary statistics of $\sigma_u$ and the
posterior density curves of $\sigma_u$. According to the posterior
density curve for the standard deviation $\sigma_u$, we can see that the
posterior distribution for $\sigma_u$ is a right-skewed distribution. In
addition, we also obtain a summary statistics about the posterior mean
and standard deviation for $\sigma_u$, which is 0.116921 and 0.0203579
respectively. Not only that, we also obtain the 2.5% and 97.5% quantile
for $\sigma_u$, which is 0.0812029 and 0.161289 respectively. As we
already know, the 2.5% and 97.5% quantile for $\sigma_u$ represents the
lower and upper bound for the 95% credible interval for the standard
deviation $\sigma_u$. Therefore, we can said that the true values of the
standard deviation $\sigma_u$ will lies between 0.0812029 and 0.161289
with a probability 95%.

```{r}
# Summary statistics of sigma
marginal.tau=m3.I$marginals.hyperpar[[1]]
marginal.sigma <- inla.tmarginal(function(tau) tau^(-1/2),marginal.tau)
inla.zmarginal(marginal.sigma)

# Plot posterior density of sigma
plot(marginal.sigma,type ="l",xlab=expression(sigma),ylab="Density",
     main='Posterior density of'~sigma~'for independent model',cex.main=0.7)
```

We printed out the summary statistics of the random effect terms in the
independent model for each 49 state in the dataset. Noted that the
random effect terms is the intercept parameters for each of the 49
state. From the summary statistics of the random effect terms below, we
can see that some of the posterior standard deviation is typically
smaller than the posterior means in absolute values, indicating that the
amount of data is sufficient to fit some of the random effect terms
parameters with some degree of confidence.

```{r}
# Summary statistics for random effect
m3.I$summary.random$state
```

Now lets compute the DIC and negative log-CPO (NSLCPO). According to the
output below, we can see that the NSLCPO and DIC are 36115.83 and
72231.68 respectively. Compared to the NSLCPO and DIC of the identical
model, we can see that NSLCPO and DIC of the independent model is much
more smaller than the NSLCPO and DIC of the identical model. This
indicates that the independent model fits well on the data compared to
the identical model.

```{r}
# Compute NSLCPO
cat("NSLCPO model 3:",-sum(log(m3.I$cpo$cpo)),"\n")

# Compute DIC
cat("DIC model 3:",m3.I$dic$dic,"\n")
```

Lastly, we will perform posterior predictive checks on the posterior
predictive distribution of the proportion of defaults among clients in
each of the 49 states contained in the dataset. As the Predictor
variables in the posterior samples of INLA contain the linear predictors
$\eta_i = \text{logit}(p_i)$ where
$\eta_i=\beta_0+\sum\limits_{j=1}^{n}\beta_{j}x_{ji}$, in order to get
the posterior predictive samples of the proportion of defaults among
clients in each of the 49 states, we need to apply the inverse logit
function on these linear predictors. The logit link function is of the
form $$\text{logit}(p)=\log{\left(\dfrac{p}{1-p}\right)},$$and so the
inverse logit function is
$$\text{inv.logit}(x) = \dfrac{1}{1+e^{-x}}.$$The $\verb|R|$ code below
perform a posterior predictive check on the posterior predictive
distribution of the proportion of defaults among clients in each of the
49 states.

```{r}
# Generate replicate samples
nbsamp=100
n=nrow(lending[1:90000,])

lending.independent.samples=inla.posterior.sample(n=nbsamp, result=m3.I)

predictor.samples.independent=inla.posterior.sample.eval(function(...){Predictor},
                                                         lending.independent.samples)


# Calculate replicate probability defaulted loans
probarep2 <- rowMeans(inv.logit(predictor.samples.independent))

# Plot histogram of predictive distribution for each state

par(mfrow=c(7,7),mar=c(1,1,1,1))

for (i in unique(lending$state[1:n])){
  ind.default <- which(lending$state[1:n]==i & lending$y[1:n]==1)
  ind.state <- which(lending$state[1:n]==i)
  proba.default <- length(ind.default)/length(ind.state)
  max.vert <- max(c(proba.default,probarep2[ind.state]))
  min.vert <- min(c(proba.default,probarep2[ind.state]))
  hist(probarep2[ind.state],col="gray40",xlim=c(min.vert,max.vert),cex.main = 0.7,
         cex.axis=0.5,main=i)
  abline(v = proba.default,col='red',lwd=2)
}
```

The histograms above show posterior predictive distribution for the
proportion of defaults among clients in each of the 49 states for the
Bayesian fit to the loan status in the $\verb|lending.csv|$ dataset. The
proportion of defaults among clients in each of the 49 states in the
observed dataset are shown by vertical red lines.

Based on these plots, we can confirm that for several state (NE, IA, and
MS), the observed number of proportion of defaults among clients falls
outside the range of the replicates proportion of defaults in the
predictive distributions plot. This might be due to we only use the
first 90000 observations in the dataset instead using all observations
in the dataset.

**d)[10 marks] (Hierarchical model)**

**Using INLA, implement a Bayesian logistic GLM with y as a function of
the scaled log interest rate, scaled log annual income, scaled log loan
amount, scaled debt-to-income, and sub.grade, and an hierarchical model
for the state variable (each state has a different intercept but the
intercepts are random draws from the same distribution, see the Radon
levels example on page 38 of Lecture 5).**

**Propose your own priors for the model parameters.**

**Print out the model summary.**

**Compute DIC and negative log-CPO (NLSCPO).**

**Do posterior predictive checks with the functions being evaluated
chosen as the proportion of defaults among clients in each of the 49
states contained in the dataset. Discuss the model fit.**

Explanation:

In this part, we are going to implement the hierarchical model, which in
this case is a Bayesian logistic GLM where each state has a different
intercept but the intercepts are random draws from the same
distribution. This can be achieved using random effects in INLA by
adding $\verb|1|$ and
$$\verb|f(state,model="iid",hyper=prec.prior.random.eff)|$$into the
model formula that we are provide to INLA in our $\verb|R|$ code, with
the appropriate prior distribution set by the $\verb|hyper|$ attribute.
Note that the common mean of the random intercepts is achieved by having
a "global" intercept in this model, hence we specified it to be 1 in the
formula. The appropriate priors on the hyperparameters are set by
choosing $\verb|prec.prior.random.eff|$, and also set the
$\verb|control.family|$ equals to $\verb|list(hyper=prec.prior)|$ along
with set the $\verb|control.fixed|$ equals to $\verb|prior.beta|$
options. The reference that we used for understanding the logistic GLM
hierarchical model was inspired from this following link:
[https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html](https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#sec:binarydata).

Now, we will specify the prior that we are going to use in INLA. For the
regression parameters, we will use the same Gaussian prior in part b).
Thus the Gaussian prior for the regression parameters in our independent
model is $\text{N}\left(0,100\right)$.

Next, we will specify the prior for the random effect terms $u_j$, where
$j = 1,2,\dots,49$. We will used the Gaussian prior with mean 0 and
variance $\sigma^2_u$ for the random effect terms $u_j$ just like in
part c). This time we let the prior distribution for $\sigma_u$ be a
Uniform distribution $U[0,20]$ to ensure that $\sigma_u$ become much
more larger, which will ensure that the probability of a loan status
being charge-off is smaller than the probability of a loan status being
fully-paid. In this part, we will use the first 90000 first observations
in the $\verb|lending.csv|$ dataset to ensure that INLA would not
crashed and also not produced error when we fit too much dataset. The
$\verb|R|$ code below implement the hierarchical model in INLA.

```{r}
sigma.unif.prior.random.eff = "expression:
  b = 20;
  log_dens = (theta>=(-2*log(b)))*(-log(b)-theta/2-log(2)) + (theta<(-2*log(b)))*(-Inf);
  return(log_dens);"

b=20;


#We select the prior for the regression coefficients
prior.beta <- list(mean.intercept = 0, prec.intercept = 0.01,mean = 0, prec = 0.01)

#The hyperparameter precision of the random effect is stored on the log-scale, 
#and it has to be input in this form when specifying it
#fixed=TRUE ensures that it is fixed at its initial value
prec.prior.random.eff <- list(prec=list(prior=sigma.unif.prior.random.eff,
                                        initial=(-2*log(b)+1),fixed = FALSE))

# Fit the logistc GLM hierarchical model into INLA
m4.I<-inla(y~1+log.interest.rate+log.annual.income+log.loan.amount+debt.to.income+sub.grade
           +term+f(state,model="iid",hyper=prec.prior.random.eff),
           data=lending[1:90000,],family ="binomial",Ntrials=1,
           control.family=list(link="logit"),control.fixed=prior.beta,
           control.compute=list(waic=T,config=T,cpo=T,dic=T))

# Display model summary
summary(m4.I)
```

As we can see from the model summary output above, the column
$\verb|0.025quant|$ and $\verb|0.975quant|$ represents the lower and
upper bound of a 95% credible interval for our regression parameter. In
Bayesian statistics, we can interpret the 95% credible interval for the
regression parameter as a fix interval where the value of interest of
our regression parameter will lies on that interval with probability
95%.

If we observe the output of the hierarchical model that we obtained from
INLA using the $\verb|summary(m4.I)|$ command, we can see that the
values of the posterior mean and standard deviation for the posterior
distribution of the regression parameter coefficient for the explanatory
variable that we obtained using the $\verb|summary(m4.I)|$ command is
significantly different from zero, which implies that all the
explanatory variable that we used in the hierarchical model is most
likely important in the model.

Next, we printed out the summary statistics of $\sigma_u$ and the
posterior density curves of $\sigma_u$. According to the posterior
density curve for the standard deviation $\sigma_u$, we can see that the
posterior distribution for $\sigma_u$ is a right-skewed distribution. In
addition, we also obtain a summary statistics about the posterior mean
and standard deviation for $\sigma_u$, which is 0.117008 and 0.0204664
respectively. Not only that, we also obtain the 2.5% and 97.5% quantile
for $\sigma_u$, which is 0.081166 and 0.16151 respectively. As we
already know, the 2.5% and 97.5% quantile for $\sigma_u$ represents the
lower and upper bound for the 95% credible interval for the standard
deviation $\sigma_u$. Therefore, we can said that the true values of the
standard deviation $\sigma_u$ will lies between 0.081166 and 0.16151
with a probability 95%.

```{r}
# Summary statistics of sigma
marginal.tau=m4.I$marginals.hyperpar[[1]]
marginal.sigma <- inla.tmarginal(function(tau) tau^(-1/2),marginal.tau)
inla.zmarginal(marginal.sigma)

# Plot posterior density of sigma
plot(marginal.sigma,type ="l",xlab=expression(sigma),ylab="Density",
     main='Posterior density of'~sigma~'for hierarchical model',cex.main=0.7)
```

We printed out the summary statistics of the random effect terms in the
hierarchical model for each 49 state in the dataset. Noted that the
random effect terms is the intercept parameters for each of the 49
state. From the summary statistics of the random effect terms below, we
can see that some of the posterior standard deviation is typically
smaller than the posterior means in absolute values, indicating that the
amount of data is sufficient to fit some of the random effect terms
parameters with some degree of confidence.

```{r}
# Summary statistics for random effect
m4.I$summary.random$state
```

Now lets compute the DIC and negative log-CPO (NSLCPO). According to the
output below, we can see that the NSLCPO and DIC are 36115.78 and
72231.59 respectively. Compared to the NSLCPO and DIC of the identical
model and independent model, we can see that NSLCPO and DIC of the
hierarchical model is much more smaller than the NSLCPO and DIC of the
identical model and independent model. This indicates that the
hierarchical model fits well on the data compared to the identical model
and independent model.

```{r}
# Compute NSLCPO
cat("NSLCPO model 4:",-sum(log(m4.I$cpo$cpo)),"\n")

# Compute DIC
cat("DIC model 4:",m4.I$dic$dic,"\n")
```

Lastly, we will perform posterior predictive checks on the posterior
predictive distribution of the proportion of defaults among clients in
each of the 49 states contained in the dataset. As the Predictor
variables in the posterior samples of INLA contain the linear predictors
$\eta_i = \text{logit}(p_i)$ where
$\eta_i=\beta_0+\sum\limits_{j=1}^{n}\beta_{j}x_{ji}$, in order to get
the posterior predictive samples of the proportion of defaults among
clients in each of the 49 states, we need to apply the inverse logit
function on these linear predictors. The logit link function is of the
form $$\text{logit}(p)=\log{\left(\dfrac{p}{1-p}\right)},$$and so the
inverse logit function is
$$\text{inv.logit}(x) = \dfrac{1}{1+e^{-x}}.$$The $\verb|R|$ code below
perform a posterior predictive check on the posterior predictive
distribution of the proportion of defaults among clients in each of the
49 states.

```{r}
# Generate replicate samples
nbsamp=100
n=nrow(lending[1:90000,])

lending.hierarchical.samples=inla.posterior.sample(n=nbsamp, result=m4.I)

predictor.samples.hierarchical=inla.posterior.sample.eval(function(...) {Predictor},
                                                       lending.hierarchical.samples)

# Calculate replicate probability defaulted loan
probarep3 <- rowMeans(inv.logit(predictor.samples.hierarchical))

# Plot histogram of predictive distribution for each state

par(mfrow=c(7,7),mar=c(1,1,1,1))

for (i in unique(lending$state[1:n])){
  ind.default <- which(lending$state[1:n]==i & lending$y[1:n]==1)
  ind.state <- which(lending$state[1:n]==i)
  proba.default <- length(ind.default)/length(ind.state)
  max.vert <- max(c(proba.default,probarep3[ind.state]))
  min.vert <- min(c(proba.default,probarep3[ind.state]))
  hist(probarep3[ind.state],col="gray40",xlim=c(min.vert,max.vert),cex.main = 0.7,
         cex.axis=0.5,main=i)
  abline(v = proba.default,col='red',lwd=2)
}
```

The histograms above show posterior predictive distribution for the
proportion of defaults among clients in each of the 49 states for the
Bayesian fit to the loan status in the $\verb|lending.csv|$ dataset. The
proportion of defaults among clients in each of the 49 states in the
observed dataset are shown by vertical red lines.

Based on these plots, we can confirm that for several state (NE, IA, and
MS), the observed number of proportion of defaults among clients falls
outside the range of the replicates proportion of defaults in the
predictive distributions plot. This might be due to we only use the
first 90000 observations in the dataset instead using all observations
in the dataset.

**e)[10 marks] Prediction of probability of default based on salary**

**Consider a set of clients with loan amount of 20000\$, interest rate
of 15%, sub grade of B1, debt to income of 10%, state of "CA"
(California), term of 60 months, and annual incomes of**

**40000, 50000, 60000, ... , 300000\$.**

**Plot the posterior means of the probabilities of default on the loans
in function of the annual incomes based on the hierarchical model from
part d).**

Explanation:

In this part we are going to predict the probability of default based on
salary. First we will store the new data containing the loan amount,
interest rate, sub grade, debt to income, state, term, and loan amount
in a data frame called $\verb|lending2|$. Then we add a new column of
loan status to the $\verb|lending2|$ data where this columns contains
all $\verb|NA|$ values. After we create the data that we want to predict
the probability of defaulted loans in each observations, we then load
the $\verb|lending.csv|$ data into $\verb|R|$ as a data frame called
$\verb|lending1|$. Then we combine the $\verb|lending2|$ and
$\verb|lending1|$ data frame into a single data frame called
$\verb|lending3|$. Before we fit our hierarchical model in part d) using
the $\verb|lending3|$ data, we need to do some data manipulation.

First, we will convert the loan status variable into a binary variable
by converting the loan status "Charged Off" into 1 and the loan status
"Fully Paid" into 0. Next, we will convert the interest rate and
debt-to-income ratio by removing the "%" pattern from the data using
$\verb|str_replace_all()|$ function in $\verb|R|$ and then convert the
variable into a numeric variable by using the $\verb|as.numeric()|$
function in $\verb|R|$. After that, we calculate the logarithm of the
loan amount, the logarithm of the interest rate, and the logarithm of
the annual income using the $\verb|log()|$ function in $\verb|R|$. We
add this three variable to the data and scale the data (centering the
data by the means then divide by the standard deviations) using the
$\verb|scale()|$ function in $\verb|R|$. In addition, we will also used
the $\verb|scale()|$ function on the debt-to-income data but we would
not use log transformations on these data.

Next, we will fit our logistic hierarchical model in part d) using the
first 90027 observations in the $\verb|lending3|$ dataset to ensure that
INLA would not crashed and also not produced error when we fit too much
dataset. The reason we used the first 90027 dataset is because we put
all the observations of the $\verb|lending2|$ dataset as our first 27
observations in the $\verb|lending3|$ dataset and the next 90000
observations after the 27th observations is the original dataset that we
used to build our hierarchical model in part d). As for the prior of the
regression parameters and random effect terms that we are going to use
here, we will use the same prior that we proposed in part d).

Lastly, we will perform posterior predictive sampling and then used the
inverse logistic function to on the our posterior predictive sampling to
obtain the posterior probability of a loan being defaulted sample. We
then use the $\verb|rowMeans()|$ function in $\verb|R|$ to compute the
posterior mean of the probability of the loan being defaulted.
Subsequently, we will provide the results in a line chart of annual
incomes against the posterior mean of the probability of the loan being
defaulted along with the 95% credible interval of the probability of the
loan being defaulted. The $\verb|R|$ code below implement the entire
data analysis procedure that we explained in this part.

```{r}
# Write data frame to predict
lending2 = data.frame(loan.amount=rep(20000,27),
                      interest.rate=rep('15%',27),
                      sub.grade=rep('B1',27),
                      annual.income=seq(40000,300000,10000),
                      debt.to.income=rep('10%',27),
                      loan.status=rep('NA',27),
                      state=rep('CA',27),
                      term=rep(" 60 months",27))

# Read original data frame
lending1 = read.csv(file='lending.csv')

# Combine two data frame
lending3 = rbind(lending2,lending1)

# Create response column y
lending3$y <- ifelse(lending3$loan.status=="Charged Off",1,0)

# Extract numbers from interest rate and debt-to-income
lending3$interest.rate <- as.numeric(str_replace_all(lending3$interest.rate,"%",""))
lending3$debt.to.income <- as.numeric(str_replace_all(lending3$debt.to.income,"%",""))

# Create logarithm on loan amount, interest rate, and annual income
lending3$log.loan.amount <- log(lending3$loan.amount)
lending3$log.interest.rate <- log(lending3$interest.rate)
lending3$log.annual.income <- log(lending3$annual.income)

# Scale the converted log column and debt-to-income
lending3$log.loan.amount <- scale(lending3$log.loan.amount)
lending3$log.interest.rate <- scale(lending3$log.interest.rate)
lending3$log.annual.income <- scale(lending3$log.annual.income)
lending3$debt.to.income <- scale(lending3$debt.to.income)

# Prior for random effects term
sigma.unif.prior.random.eff = "expression:
  b = 20;
  log_dens = (theta>=(-2*log(b)))*(-log(b)-theta/2-log(2)) + (theta<(-2*log(b)))*(-Inf);
  return(log_dens);"

b=20;


#We select the prior for the regression coefficients
prior.beta <- list(mean.intercept = 0, prec.intercept = 0.01,mean = 0, prec = 0.01)

#The hyperparameter precision of the random effect is stored on the log-scale, 
#and it has to be input in this form when specifying it
#fixed=TRUE ensures that it is fixed at its initial value
prec.prior.random.eff <- list(prec=list(prior=sigma.unif.prior.random.eff,
                                        initial=(-2*log(b)+1),fixed = FALSE))

# Fit the logistc GLM hierarchical model into INLA
m5.I<-inla(y~1+log.interest.rate+log.annual.income+log.loan.amount+debt.to.income+sub.grade
           +term+f(state,model="iid",hyper=prec.prior.random.eff),
           data=lending3[1:90027,],family ="binomial",Ntrials=1,
           control.predictor=list(compute=T),control.family=list(link="logit"),
           control.fixed=prior.beta,
           control.compute=list(waic=T,config=T,cpo=T,dic=T))

# Generate replicate samples
nbsamp=100

hierarchical.samples=inla.posterior.sample(n=nbsamp, result=m5.I,
                                           selection = list(Predictor=1:27))

predictor.samples=inla.posterior.sample.eval(function(...) {Predictor},
                                             hierarchical.samples)

# Calculate posterior means probability defaulted loan
post.mean = rowMeans(inv.logit(predictor.samples))

# Calculate 95% credible interval for probability defaulted loans
post.q025=apply(inv.logit(predictor.samples), MARGIN=1,FUN=function(x) quantile(x,prob=0.025))

post.q975=apply(inv.logit(predictor.samples),MARGIN=1,FUN=function(x) quantile(x,prob=0.975))

# Plot annual income vs posterior means 
lower.vert = min(c(post.q025,post.mean,post.q975))
upper.vert = max(c(post.q025,post.mean,post.q975))
vert.lim = c(lower.vert,upper.vert)
plot(lending2$annual.income,post.mean,type='l',col='red',xlab='Annual Incomes($)',
     ylab='Probability of Defaulted Loans',cex.lab=0.7,cex.main=0.7,
     ylim=vert.lim,
     main='Probability of Defaulted Loans Based on Salary')

# Plot 95% Credible interval for probability of defaulted loans
lines(lending2$annual.income,post.q025,lty=2,col="dark red")
lines(lending2$annual.income,post.q975,lty=3,col="dark red")

# Add legend to plot
legend("topright",legend=c("2.5% quantile","posterior mean","97.5% quantile"),
       col=c("dark red","red","dark red"),lty=c(2,1,3),bty='n',cex=0.7)
```

From the plot above, we can see that the probability of a loan being
defaulted decreased as the annual income increases. Based on this plot,
we can said that the client from California with annual income \$300000
have small chances for having a defaulted loans compared to other 26
clients from California who have annual income between \$50000 and
\$290000.
